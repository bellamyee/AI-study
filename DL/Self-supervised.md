Recent Trends Transformer

â€¢ Transformer model and its self-attention block has become a general-purpose sequence (or
set) encoder and decoder in recent NLP applications as well as in other areas.
â€¢ Training deeply stacked Transformer models via a self-supervised learning framework has
significantly advanced various NLP tasks through transfer learning, e.g., BERT, GPT-3, XLNet,
ALBERT, RoBERTa, Reformer, T5, ELECTRAâ€¦
â€¢ Other applications are fast adopting the self-attention and Transformer architecture as well as
self-supervised learning approach, e.g., recommender systems, drug discovery, computer
vision, â€¦
â€¢ As for natural language generation, self-attention models still requires a greedy decoding of
words one at a time.
Attention Is All You Need, NeurIPSâ€™17

â“’ NAVER Connect Foundation
GPT-1
4
â“’ NAVER Connect Foundation
Improving Language Understanding by Generative Pre-training
5
â€¢ GPT-1
â€¢ It introduces special tokens, such as <S> /<E>/ $, to achieve effective transfer learning
during fine-tuning
â€¢ It does not need to use additional task-specific architectures on top of transferred
representations
â€¢ 12-layer decoder-only transformer
â€¢ 12 head / 768 dimensional states
â€¢ GELU activation unit
https://blog.openai.com/language-unsupervised/
â“’ NAVER Connect Foundation 6
â€¢ Experimental Results
Improving Language Understanding by Generative Pre-training
https://blog.openai.com/language-unsupervised/
â“’ NAVER Connect Foundation
BERT
7
â“’ NAVER Connect Foundation 8
â€¢ Learn through masked language modeling task
â€¢ Use large-scale data and large-scale model
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Unidirectional Bi-LSTM
BERT: Pre-training of deep bidirectional transformers for language understanding, NAACLâ€™19
â“’ NAVER Connect Foundation 9
â€¢ Motivation
â€¢ Language models only use left context or right context, but language understanding is bidirectional
â€¢ If we use bi-directional language model?
â€¢ Problem: Words can â€œsee themselvesâ€ (cheating) in a bi-directional encoder
Masked Language Model
â“’ NAVER Connect Foundation 10
â€¢ Masked Language Model (MLM)
â€“ Mask some percentage of the input tokens at random, and then predict those masked
tokens.
â€“ 15% of the words to predict
â€¢ 80% of the time, replace with [MASK]
â€¢ 10% of the time, replace with a random word
â€¢ 10% of the time, keep the sentence as same
â€¢ Next Sentence Prediction (NSP)
â€“ Predict whether Sentence B is an actual sentence that proceeds Sentence A, or a random
sentence
Pre-training Tasks in BERT
https://nlp.stanford.edu/seminar/details/jdevlin.pdf
â“’ NAVER Connect Foundation 11
â€¢ How to
â€¢ Mask out k% of the input words, and then predict the masked words
â€¢ e.g., use k= 15%
â€¢ Too little masking : Too expensive to train
â€¢ Too much masking : Not enough to capture context
Pre-training Tasks in BERT: Masked Language Model
https://nlp.stanford.edu/seminar/details/jdevlin.pdf
â“’ NAVER Connect Foundation 12
â€¢ Problem
â€¢ Mask token never seen during fine-tuning
â€¢ Solution
â€¢ 15% of the words to predict, but donâ€™t replace with [MASK] 100% of the time. Instead:
â€¢ 80% of the time, replace with [MASK]
â€“ went to the store â†’ went to the [MASK]
â€¢ 10% of the time, replace with a random word
â€“ went to the store â†’ went to the running
â€¢ 10% of the time, keep the same sentence
â€“ went to the store â†’ went to the store
Pre-training Tasks in BERT: Masked Language Model
â“’ NAVER Connect Foundation 13
â€¢ To learn the relationships among sentences, predict whether Sentence B is an actual
sentence that proceeds Sentence A, or a random sentence
Pre-training Tasks in BERT: Next Sentence Prediction
https://nlp.stanford.edu/seminar/details/jdevlin.pdf
â“’ NAVER Connect Foundation 14
1. Model Architecture
â€“ BERT BASE: L = 12, H = 768, A = 12
â€“ BERT LARGE: L = 24, H = 1024, A = 16
2. Input Representation
â€“ WordPiece embeddings (30,000 WordPiece)
â€“ Learned positional embedding
â€“ [CLS] â€“ Classification embedding
â€“ Packed sentence embedding [SEP]
â€“ Segment Embedding
3. Pre-training Tasks
â€“ Masked LM
â€“ Next Sentence Prediction
BERT Summary
â“’ NAVER Connect Foundation
Transformer: Positional Encoding Transformer
15
â€¢ Use sinusoidal functions of different frequencies
ğ‘ƒğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–) = sin(ğ‘ğ‘œğ‘ /100002ğ‘–/ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™)
ğ‘ƒğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–+1) = cos(ğ‘ğ‘œğ‘ /100002ğ‘–/ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™)
â€¢ Easily learn to attend by relative position, since for any fixed offset ğ‘˜,
ğ‘ƒğ¸(ğ‘ğ‘œğ‘ +ğ‘˜) can be represented as linear function of ğ‘ƒğ¸ ğ‘ğ‘œğ‘ 
http://nlp.seas.harvard.edu/2018/04/03/attention
â“’ NAVER Connect Foundation 16
BERT: Input Representation
BERT: Pre-training of deep bidirectional transformers for language understanding, NAACLâ€™19
â€¢ The input embedding is the sum of the token embeddings, the segmentation embeddings
and the position embeddings
â“’ NAVER Connect Foundation 17
â€¢ Learn through masked language modeling task
â€¢ Use large-scale data and large-scale model
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Unidirectional Bi-LSTM
BERT: Pre-training of deep bidirectional transformers for language understanding, NAACLâ€™19
â“’ NAVER Connect Foundation 18
BERT: Fine-tuning Process
BERT: Pre-training of deep bidirectional transformers for language understanding, NAACLâ€™19
â€¢ Transfer Learning
https://blog.openai.com/language-unsupervised/
â“’ NAVER Connect Foundation 19
BERT vs GPT-1
â€¢ Comparison of BERT and GPT-1
â€¢ Training-data size
â€¢ GPT is trained on BookCorpus(800M words) ; BERT is trained on the BookCorpus and
Wikipedia (2,500M words)
â€¢ Training special tokens during training
â€¢ BERT learns [SEP],[CLS], and sentence A/B embedding during pre-training
â€¢ Batch size
â€¢ BERT â€“ 128,000 words ; GPT â€“ 32,000 words
â€¢ Task-specific fine-tuning
â€¢ GPT uses the same learning rate of 5e-5 for all fine-tuning experiments; BERT
chooses a task-specific fine-tuning learning rate.
â“’ NAVER Connect Foundation 20
BERT: GLUE Benchmark Results
â€¢ GLUE Benchmark Results
BERT: Pre-training of deep bidirectional transformers for language understanding, NAACLâ€™19
â“’ NAVER Connect Foundation 21
Machine Reading Comprehension (MRC), Question Answering
â“’ NAVER Connect Foundation 22
BERT: SQuAD 1.1
Only new parameters: Start vector and end vector
https://rajpurkar.github.io/SQuAD-explorer/
â“’ NAVER Connect Foundation 23
BERT: SQuAD 2.0
â€¢ Use token 0 ([CLS]) to emit logit for â€œno answerâ€
â€¢ â€œNo answerâ€ directly competes with answer span
â€¢ Threshold is optimized on dev set
https://rajpurkar.github.io/SQuAD-explorer/
â“’ NAVER Connect Foundation 24
BERT: On SWAG
â€¢ Run each Premise + Ending through BERT
â€¢ Produce logit for each pair on token 0 ([CLS])
https://leaderboard.allenai.org/swag/submissions/public
â“’ NAVER Connect Foundation 25
BERT: Ablation Study
â€¢ Big models help a lot
â€¢ Going from 110M to 340M params helps even on datasets with 3,600 labeled
examples
â€¢ Improvements have not asymptoted
BERT: Pre-training of deep bidirectional transformers for language understanding, NAACLâ€™19
â“’ NAVER Connect Foundation
2.
Advanced Self-supervised Pre-training Models
26
GPT-2
GPT-3
ALBERT
ELECTRA
Light-weight Models
Fusing Knowledge Graph into Language Model
â“’ NAVER Connect Foundation
GPT-2
27
â“’ NAVER Connect Foundation 28
GPT-2: Language Models are Unsupervised Multi-task Learners
â€¢ Just a really big transformer LM
â€¢ Trained on 40GB of text
â€¢ Quite a bit of effort going into making sure the dataset is good quality
â€¢ Take webpages from reddit links with high karma
â€¢ Language model can perform down-stream tasks in a zero-shot setting â€“ without any
parameter or architecture modification
â“’ NAVER Connect Foundation 29
GPT-2: Language Models are Unsupervised Multi-task Learners
https://blog.floydhub.com/gpt2/
â“’ NAVER Connect Foundation 30
GPT-2: Motivation (decaNLP)
â€¢ The Natural Language Decathlon: Multitask Learning as Question Answering
â€¢ Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher
https://decanlp.com/
â“’ NAVER Connect Foundation 31
GPT-2: Datasets
â€¢ A promising source of diverse and nearly unlimited text is web scrape
such as common crawl
â€¢ They scraped all outbound links from Reddit, a social media platform, WebText
â€¢ 45M links
â€¢ Scraped web pages which have been curated/filtered by humans
â€¢ Received at least 3 karma (up-vote)
â€¢ 8M removed Wikipedia documents
â€¢ Use dragnet and newspaper to extract content from links
â“’ NAVER Connect Foundation 32
GPT-2: Datasets
â€¢ Preprocess
â€¢ Byte pair encoding (BPE)
â€¢ Minimal fragmentation of words across multiple vocab tokens
â“’ NAVER Connect Foundation 33
GPT-2: Model
â€¢ Modification
â€¢ Layer normalization was moved to the input of each sub-block, similar to a pre-activation
residual network
â€¢ Additional layer normalization was added after the final self-attention block.
â€¢ Scaled the weights of residual layer at initialization by a factor of àµ—
1
ğ‘›
where ğ‘› is the
number of residual layer
https://openai.com/blog/better-language-models/
â“’ NAVER Connect Foundation 34
GPT-2: Question Answering
â€¢ Use conversation question answering dataset(CoQA)
â€¢ Achieved 55 F1 score, exceeding the performance 3 out of 4 baselines
without labeled dataset
â€¢ Fine-tuned BERT achieved 89 F1 performance
â“’ NAVER Connect Foundation 35
GPT-2: Summarization
â€¢ CNN and Daily Mail Dataset
â€¢ Add text TL;DR: after the article and generate 100 tokens
â€¢ (TL;DR: Too long, didnâ€™t read)
https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
â“’ NAVER Connect Foundation 36
GPT-2: Translation
â€¢ User WMT14 en-fr dataset for evaluation
â€¢ Use LMs on a context of example pairs of
the format:
â€¢ English sentence = French sentence
â€¢ Achieve 5 BLEU score in word-by-word
substitution
â€¢ Slightly worse than MUSE (Conneau et al., 2017)
https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
â“’ NAVER Connect Foundation
GPT-3
37
â“’ NAVER Connect Foundation 38
GPT-3: Language Models are Few-Shot Learners
Language Models are Few-shot Learners
â€¢ Scaling up language models greatly improves task-agnostic, few-shot performance
â€¢ An autoregressive language model with 175 billion parameters in the few-shot setting
â€¢ 96 Attention layers, Batch size of 3.2M
0
30
60
90
120
150
180
BERT GPT-2 T5 GPT-3
Number of Parameters (B)
Model size
â“’ NAVER Connect Foundation 39
GPT-3: Language Models are Few-Shot Learners
Language Models are Few-shot Learners
â€¢ Prompt: the prefix given to the model
â€¢ Zero-shot: Predict the answer given only a natural language description of the task
â€¢ One-shot: See a single example of the task in addition to the task description
â€¢ Few-shot: See a few examples of the task
Zero-shot One-shot Few-shot
Language Models are Few-show Learners, NeurIPSâ€™20
â“’ NAVER Connect Foundation 40
GPT-3: Language Models are Few-Shot Learners
Language Models are Few-shot Learners
â€¢ Zero-shot performance improves steadily with model size
â€¢ Few-shot performance increases more rapidly
Language Models are Few-show Learners, NeurIPSâ€™20
â“’ NAVER Connect Foundation 41
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
â€¢ Is having better NLP models as easy as having larger models?
â€¢ Obstacles
â€¢ Memory Limitation
â€¢ Training Speed
â€¢ Solutions
â€¢ Factorized Embedding Parameterization
â€¢ Cross-layer Parameter Sharing
â€¢ (For Performance) Sentence Order Prediction
â“’ NAVER Connect Foundation 42
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
â€¢ Factorized Embedding Parameterization
Dimension size should be same in BERT
http://jalammar.github.io/illustrated-transformer/
â“’ NAVER Connect Foundation 43
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
â€¢ Factorized Embedding Parameterization
â€¢ V = Vocabulary size
â€¢ H = Hidden-state dimension
â€¢ E = Word embedding dimension
http://jalammar.github.io/illustrated-transformer/
x
V x E
E x H
BERT ALBERT
V x H
â“’ NAVER Connect Foundation 44
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
â€¢ Cross-layer Parameter Sharing
â€¢ Shared-FFN: Only sharing feed-forward network parameters across layers
â€¢ Shared-attention: Only sharing attention parameters across layers
â€¢ All-shared: Both of them
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ICLRâ€™20
â“’ NAVER Connect Foundation 45
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
â€¢ Sentence Order Prediction
â€¢ Next Sentence Prediction pretraining task in BERT is too easy
â€¢ Predict the ordering of two consecutive segments of text
â€¢ Negative samples the same two consecutive segments
but with their order swapped
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ICLRâ€™20
â“’ NAVER Connect Foundation 46
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
â€¢ GLUE Results
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ICLRâ€™20
â“’ NAVER Connect Foundation 47
ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately
â€¢ Efficiently Learning an Encoder that Classifies Token Replacements Accurately
â€¢ Learn to distinguish real input tokens from plausible but synthetically generated replacements
â€¢ Pre-training text encoders as discriminators rather than generators
â€¢ Discriminator is the main networks for pre-training.
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLRâ€™20
â“’ NAVER Connect Foundation 48
ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately
â€¢ Replaced token detection pre-training vs masked language model pre-training
â€¢ Outperforms MLM-based methods such as BERT given the same model size, data, and
compute
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLRâ€™20
â“’ NAVER Connect Foundation 49
Light-weight Models
â€¢ DistillBERT (NeurIPS 2019 Workshop)
â€¢ A triple loss, which is a distillation loss over the soft target probabilities of the
teacher model leveraging the full teacher distribution
â€¢ TinyBERT (Findings of EMNLP 2020)
â€¢ Two-stage learning framework, which performs Transformer distillation at both
the pre-training and task-specific learning stages
â“’ NAVER Connect Foundation 50
Fusing Knowledge Graph into Language Model
â€¢ ERNIE: Enhanced Language Representation with Informative Entities (ACL 2019)
â€¢ Informative entities in a knowledge graph enhance language representation
â€¢ Information fusion layer takes the concatenation of the token embedding and
entity embedding
â€¢ KagNET: Knowledge-Aware Graph Networks for Commonsense Reasoning
(EMNLP 2019)
â€¢ A knowledge-aware reasoning framework for learning to answer commonsense
questions
â€¢ For each pair of question and answer candidate, it retrieves a sub-graph from an
external knowledge graph to capture relevant knowledge
â“’ NAVER Connect Foundation
References
51
â€¢ GPT-1
â€¢ https://blog.openai.com/language-unsupervised/
â€¢ BERT : Pre-training of deep bidirectional transformers for language understanding, NAACLâ€™19
â€¢ https://arxiv.org/abs/1810.04805
â€¢ SQuAD: Stanford Question Answering Dataset
â€¢ https://rajpurkar.github.io/SQuAD-explorer/
â€¢ SWAG: A Large-scale Adversarial Dataset for Grounded Commonsense Inference
â€¢ https://leaderboard.allenai.org/swag/submissions/public
â€¢ How to Build OpenAIâ€™s GPT-2: â€œ The AI That Was Too Dangerous to Releaseâ€
â€¢ https://blog.floydhub.com/gpt2/
â“’ NAVER Connect Foundation
References
52
â€¢ GPT-2
â€¢ https://openai.com/blog/better-language-models/
â€¢ https://cdn.openai.com/better-language
models/language_models_are_unsupervised_multitask_learners.pdf
â€¢ Language Models are Few-shot Learners, NeurIPSâ€™20
â€¢ https://arxiv.org/abs/2005.14165
â€¢ Illustrated Transformer
â€¢ http://jalammar.github.io/illustrated-transformer/
â€¢ ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ICLRâ€™20
â€¢ https://arxiv.org/abs/1909.11942
â€¢ ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLRâ€™20
â€¢ https://arxiv.org/abs/2003.10555
â“’ NAVER Connect Foundation
References
53
â€¢ DistillBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
â€¢ https://arxiv.org/abs/1910.01108
â€¢ TinyBERT: Distilling BERT for Natural Language Understanding, Findings of EMNLPâ€™20
â€¢ https://arxiv.org/abs/1909.10351
â€¢ ERNIE: Enhanced Language Representation with Informative Entities
â€¢ https://arxiv.org/abs/1905.07129
â€¢ KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning
â€¢ https://arxiv.org/abs/1909.02151
